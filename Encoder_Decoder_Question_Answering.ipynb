{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Chatbot with Tensorflow using MetaLWOz Dataset\n",
        "A chatbot trained on 37844 crosstalk pairs. The dataset used is the [MetaLWOz](https://www.microsoft.com/en-us/research/project/metalwoz/)"
      ],
      "metadata": {
        "id": "hBQDM6CJI8_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A) Import libraries"
      ],
      "metadata": {
        "id": "IWgDA_OQJ_GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ],
      "metadata": {
        "id": "eBGorfSoJWGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B) Reading the data from the files"
      ],
      "metadata": {
        "id": "G8mHF9vtC1gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.microsoft.com/download/E/B/8/EB84CB1A-D57D-455F-B905-3ABDE80404E5/metalwoz-v1.zip -O metalwoz-v1.zip\n",
        "!unzip metalwoz-v1.zip\n",
        "dir_path = 'dialogues'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "metadata": {
        "id": "Z8qYBSUS1c9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1e9a7b-d2be-4ce0-cd81-46e250ce5ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-04 15:51:40--  https://download.microsoft.com/download/E/B/8/EB84CB1A-D57D-455F-B905-3ABDE80404E5/metalwoz-v1.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.39.1.112, 2600:1406:3c:393::317f, 2600:1406:3c:3a4::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.39.1.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5639228 (5.4M) [application/octet-stream]\n",
            "Saving to: ‘metalwoz-v1.zip’\n",
            "\n",
            "metalwoz-v1.zip     100%[===================>]   5.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-04 15:51:40 (36.9 MB/s) - ‘metalwoz-v1.zip’ saved [5639228/5639228]\n",
            "\n",
            "Archive:  metalwoz-v1.zip\n",
            "  inflating: LICENSE.pdf             \n",
            "  inflating: tasks.txt               \n",
            "  inflating: dialogues/AGREEMENT_BOT.txt  \n",
            "  inflating: dialogues/ALARM_SET.txt  \n",
            "  inflating: dialogues/APARTMENT_FINDER.txt  \n",
            "  inflating: dialogues/APPOINTMENT_REMINDER.txt  \n",
            "  inflating: dialogues/AUTO_SORT.txt  \n",
            "  inflating: dialogues/BANK_BOT.txt  \n",
            "  inflating: dialogues/BUS_SCHEDULE_BOT.txt  \n",
            "  inflating: dialogues/CATALOGUE_BOT.txt  \n",
            "  inflating: dialogues/CHECK_STATUS.txt  \n",
            "  inflating: dialogues/CITY_INFO.txt  \n",
            "  inflating: dialogues/CONTACT_MANAGER.txt  \n",
            "  inflating: dialogues/DECIDER_BOT.txt  \n",
            "  inflating: dialogues/EDIT_PLAYLIST.txt  \n",
            "  inflating: dialogues/EVENT_RESERVE.txt  \n",
            "  inflating: dialogues/GAME_RULES.txt  \n",
            "  inflating: dialogues/GEOGRAPHY.txt  \n",
            "  inflating: dialogues/GUINESS_CHECK.txt  \n",
            "  inflating: dialogues/HOME_BOT.txt  \n",
            "  inflating: dialogues/HOW_TO_BASIC.txt  \n",
            "  inflating: dialogues/INSURANCE.txt  \n",
            "  inflating: dialogues/LIBRARY_REQUEST.txt  \n",
            "  inflating: dialogues/LOOK_UP_INFO.txt  \n",
            "  inflating: dialogues/MAKE_RESTAURANT_RESERVATIONS.txt  \n",
            "  inflating: dialogues/MOVIE_LISTINGS.txt  \n",
            "  inflating: dialogues/MUSIC_SUGGESTER.txt  \n",
            "  inflating: dialogues/NAME_SUGGESTER.txt  \n",
            "  inflating: dialogues/ORDER_PIZZA.txt  \n",
            "  inflating: dialogues/PET_ADVICE.txt  \n",
            "  inflating: dialogues/PHONE_PLAN_BOT.txt  \n",
            "  inflating: dialogues/PHONE_SETTINGS.txt  \n",
            "  inflating: dialogues/PLAY_TIMES.txt  \n",
            "  inflating: dialogues/POLICY_BOT.txt  \n",
            "  inflating: dialogues/PRESENT_IDEAS.txt  \n",
            "  inflating: dialogues/PROMPT_GENERATOR.txt  \n",
            "  inflating: dialogues/QUOTE_OF_THE_DAY_BOT.txt  \n",
            "  inflating: dialogues/RESTAURANT_PICKER.txt  \n",
            "  inflating: dialogues/SCAM_LOOKUP.txt  \n",
            "  inflating: dialogues/SHOPPING.txt  \n",
            "  inflating: dialogues/SKI_BOT.txt   \n",
            "  inflating: dialogues/SPORTS_INFO.txt  \n",
            "  inflating: dialogues/STORE_DETAILS.txt  \n",
            "  inflating: dialogues/TIME_ZONE.txt  \n",
            "  inflating: dialogues/UPDATE_CALENDAR.txt  \n",
            "  inflating: dialogues/UPDATE_CONTACT.txt  \n",
            "  inflating: dialogues/WEATHER_CHECK.txt  \n",
            "  inflating: dialogues/WEDDING_PLANNER.txt  \n",
            "  inflating: dialogues/WHAT_IS_IT.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dialogues file contains 47 .txt files which are the topics. However, it will be convenient to consider these files as JSON files with missing commmas between the JSON objects. All the JSON objects will be parsed and stored into one variable, `parsed_objects`"
      ],
      "metadata": {
        "id": "f_GTtrRQ97yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But using all 47 topics overloads the RAM (>30 GB for the creation of `decoder_output_data` in the next session) so only 3 files will be used."
      ],
      "metadata": {
        "id": "fTzVrUY-sx2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_list=files_list[0:3]\n",
        "print(files_list)"
      ],
      "metadata": {
        "id": "Oy-uuh8m7tGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8286689-f7f6-4d5f-d7f7-eb95a33821f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PHONE_PLAN_BOT.txt', 'ALARM_SET.txt', 'PET_ADVICE.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/54663739/how-to-analyze-json-objects-that-are-not-separated-by-comma-preferably-in-pytho\n",
        "\n",
        "def parse_unformatted_json(files_list, dir_path):\n",
        "  decoder = json.JSONDecoder()\n",
        "  parsed_objects = []\n",
        "\n",
        "  for topic in files_list: # for all 47 topics\n",
        "    \n",
        "    with open(dir_path + \"/\" + topic, \"r\") as f:\n",
        "        content = f.read()\n",
        "    while content:\n",
        "        value, new_start = decoder.raw_decode(content)\n",
        "        content = content[new_start:].strip()\n",
        "        parsed_objects.append(value)\n",
        "\n",
        "  return parsed_objects\n",
        "\n",
        "parsed_objects = parse_unformatted_json(files_list, dir_path)"
      ],
      "metadata": {
        "id": "8xie98F37nnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of dialogues =\", len(parsed_objects))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE_sUkabAgLG",
        "outputId": "f72e232f-c5af-49c7-acd2-d002946270d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of dialogues = 2601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example instance of parsed_objects, the values of `turns` are going to be extracted. The `turns` section always starts with the bot's greeting line, which is going to be removed later. "
      ],
      "metadata": {
        "id": "xEJGMSRcE_Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_objects[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqpOTJ6FAhru",
        "outputId": "8bca4ba3-7efe-4e95-9581-ac7ff4fba281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5a0fafb4',\n",
              " 'user_id': '62edbdf3',\n",
              " 'bot_id': '5b89b8eb',\n",
              " 'domain': 'PHONE_PLAN_BOT',\n",
              " 'task_id': '105bb6ba',\n",
              " 'turns': ['Hello how may I help you?',\n",
              "  'what can you do?',\n",
              "  'I am a bot that can help you with your mobile plan issues. I can do things like give you details of the different plans.',\n",
              "  'great, i need to upgrade my plan',\n",
              "  'I can do that! Do you have a specific plan in mind?',\n",
              "  'i want one with free calling',\n",
              "  'Do you need free calling before 7 PM or 24/7 free calling?',\n",
              "  \"it must be 24/7 free calling, that's what i need\",\n",
              "  'Okay! We offer that. Can you please give me your number so I can get your account information.',\n",
              "  '555 1212',\n",
              "  'Thanks! Please provide your security question answer so I can apply changes to your account.']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract values from `turns` JSON key for all objects for all topics"
      ],
      "metadata": {
        "id": "Q4YdMyGG9D7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_dialogue_pairs(parsed_objects):\n",
        "  user=[]\n",
        "  bot_raw=[]\n",
        "  bot = list()\n",
        "\n",
        "  for v in parsed_objects:\n",
        "    if len(v['turns']) % 2 == 1:        # if user ends the conversation\n",
        "      last_line = len(v['turns'])\n",
        "    elif len(v['turns']) % 2 == 0:      # if bot ends the conversation\n",
        "      last_line = len(v['turns'])-1\n",
        "\n",
        "    for i in range(1,last_line): # start from 1 so as to not include bot's first line (the welcoming line)\n",
        "      if i % 2 == 0:\n",
        "        bot_raw.append(v['turns'][i])\n",
        "      else:\n",
        "        user.append(v['turns'][i])\n",
        "\n",
        "    # the length of dialogues should be equal, i.e. for every user's question there should be a bot's answer\n",
        "    assert (len(user)==len(bot_raw))\n",
        "  \n",
        "  for i in range(len(bot_raw)) :\n",
        "    bot.append( '<START> ' + bot_raw[i] + ' <END>' )\n",
        "\n",
        "  return (user, bot)"
      ],
      "metadata": {
        "id": "r7GjwTv-ttj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder will progress by taking the tokens it emits as inputs, so before it has emitted anything it needs a token to start with, i.e \\<START> <p>\n",
        "The \\<END> token helps decoder to emit arbitrary-length sequences. The decoder will tell us when it's done emitting tokens: without an \"end\" token, we would have no idea when the decoder is done talking to us and continuing to emit tokens will produce gibberish."
      ],
      "metadata": {
        "id": "49FeEeLGJ-Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user, bot = extract_dialogue_pairs(parsed_objects)"
      ],
      "metadata": {
        "id": "CgD1lFJxDv_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are 13561 sentence pairs\n",
        "print(len(user))\n",
        "print(len(bot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F51Q5N3I835",
        "outputId": "317212b9-6537-409d-977e-070e49231ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13561\n",
            "13561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of 5 sentence pairs between user and bot"
      ],
      "metadata": {
        "id": "T-SjDWvpJrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0a0ng7iL_eb",
        "outputId": "380bc182-67f0-4c03-d067-2be2aee04c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what can you do?',\n",
              " 'great, i need to upgrade my plan',\n",
              " 'i want one with free calling',\n",
              " \"it must be 24/7 free calling, that's what i need\",\n",
              " '555 1212']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rYGR4FPJt2p",
        "outputId": "e07b68c9-bd59-411d-b6f9-2dddf2eab42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> I am a bot that can help you with your mobile plan issues. I can do things like give you details of the different plans. <END>',\n",
              " '<START> I can do that! Do you have a specific plan in mind? <END>',\n",
              " '<START> Do you need free calling before 7 PM or 24/7 free calling? <END>',\n",
              " '<START> Okay! We offer that. Can you please give me your number so I can get your account information. <END>',\n",
              " '<START> Thanks! Please provide your security question answer so I can apply changes to your account. <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Tokenizer and load the whole vocabulary (user + bot) into it.\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(user + bot)\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "print('VOCAB SIZE : ',VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "dRwtjmjFJa5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ab59a0-d497-4685-898c-93069fb45783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE :  4713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C) Preparing data for Seq2Seq model\n",
        "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
        "\n",
        "For `encoder_input_data` : Tokenize the questions. Pad or truncate them to length `MAX_LEN`.<p>\n",
        "For `decoder_input_data` : Tokenize the answers. Pad or truncate them to length `MAX_LEN`.<p>\n",
        "For `decoder_output_data` : Tokenize the answers. Remove the first element from all the tokenized_answers. This is the <START> element which we added earlier."
      ],
      "metadata": {
        "id": "V-YcT1F1ofQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A maximum length of 30 words per sentence is used which seems to be enough, considering that the biggest question is 43 words and the biggest answer is 68 words. "
      ],
      "metadata": {
        "id": "yLoSp7jyujhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 30\n",
        "\n",
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences(user)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "encoder_input_data = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen = MAX_LEN, padding = 'post', truncating='post')\n",
        "print(encoder_input_data.shape, maxlen_questions)\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences(bot)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "decoder_input_data = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=MAX_LEN , padding='post', truncating='post')\n",
        "print(decoder_input_data.shape, maxlen_answers)\n",
        "\n",
        "# decoder_output_data\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(bot)\n",
        "\n",
        "# remove <START> tag\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# convert to 3d matrix (num_sentences x MAX_LEN x VOCAB_SIZE)\n",
        "# for every word (out of the 30) in every sentence (out of num_sentences) convert the word to its one hot encoding representation\n",
        "decoder_output_data = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "print( decoder_output_data.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7qRIejPL0Ti",
        "outputId": "45bb43ff-05af-40c8-ccb7-2c0847dc88f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13561, 30) 43\n",
            "(13561, 30) 68\n",
            "(13561, 30, 4713)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Defining the Encoder-Decoder model"
      ],
      "metadata": {
        "id": "XR-QdVNyESFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "\n",
        "\n",
        "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
        "*   Embedding layer : For converting token vectors to fix sized dense vectors.\n",
        "*   LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working : \n",
        "\n",
        "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
        "2.   The output of the Embedding layer goes to the LSTM cells which produce 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
        "3.   These states are set in the LSTM cells of the decoder ( `decoder_lstm` ).\n",
        "4.   The decoder_input_data comes in through the Embedding layer.\n",
        "5.   The decoder embeddings go to the LSTM cells ( which have as initial states the states produced by the encoder, h & c ) in order to produce seqeunces."
      ],
      "metadata": {
        "id": "akUeywuSKEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM=150\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(MAX_LEN, ))\n",
        "# use mask_zero=True to mak out the zeros from the padding\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_DIM, mask_zero=True) (encoder_inputs)\n",
        "# connect lstm layer with encoder embedding\n",
        "encoder_outputs, state_h , state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h , state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(MAX_LEN, ))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_DIM, mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
        "# use the encoder states produced before as initial input for the decoder\n",
        "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding, initial_state=encoder_states)\n",
        "# convert to probabilities\n",
        "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECpizW85Nj4c",
        "outputId": "03ef2e3c-7841-447d-dfb2-e7caf7dc6d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 30, 150)      706950      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 30, 150)      706950      ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 200),        280800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 30, 200),    280800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 30, 4713)     947313      ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,922,813\n",
            "Trainable params: 2,922,813\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Training the model"
      ],
      "metadata": {
        "id": "CJ-aXEb7Fr8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrcobB7YOMWy",
        "outputId": "81d9de11-6a6e-4ddf-81e4-3887a1ed1d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "272/272 [==============================] - 134s 459ms/step - loss: 1.7411\n",
            "Epoch 2/35\n",
            "272/272 [==============================] - 126s 463ms/step - loss: 1.3930\n",
            "Epoch 3/35\n",
            "272/272 [==============================] - 124s 456ms/step - loss: 1.2458\n",
            "Epoch 4/35\n",
            "272/272 [==============================] - 127s 466ms/step - loss: 1.1343\n",
            "Epoch 5/35\n",
            "272/272 [==============================] - 124s 458ms/step - loss: 1.0518\n",
            "Epoch 6/35\n",
            "272/272 [==============================] - 126s 465ms/step - loss: 0.9918\n",
            "Epoch 7/35\n",
            "272/272 [==============================] - 125s 459ms/step - loss: 0.9448\n",
            "Epoch 8/35\n",
            "272/272 [==============================] - 125s 461ms/step - loss: 0.9072\n",
            "Epoch 9/35\n",
            "272/272 [==============================] - 125s 461ms/step - loss: 0.8758\n",
            "Epoch 10/35\n",
            "272/272 [==============================] - 126s 464ms/step - loss: 0.8477\n",
            "Epoch 11/35\n",
            "272/272 [==============================] - 126s 464ms/step - loss: 0.8214\n",
            "Epoch 12/35\n",
            "272/272 [==============================] - 125s 458ms/step - loss: 0.7975\n",
            "Epoch 13/35\n",
            "272/272 [==============================] - 128s 470ms/step - loss: 0.7754\n",
            "Epoch 14/35\n",
            "272/272 [==============================] - 125s 460ms/step - loss: 0.7543\n",
            "Epoch 15/35\n",
            "272/272 [==============================] - 127s 467ms/step - loss: 0.7344\n",
            "Epoch 16/35\n",
            "272/272 [==============================] - 125s 458ms/step - loss: 0.7147\n",
            "Epoch 17/35\n",
            "272/272 [==============================] - 129s 474ms/step - loss: 0.6964\n",
            "Epoch 18/35\n",
            "272/272 [==============================] - 126s 462ms/step - loss: 0.6781\n",
            "Epoch 19/35\n",
            "272/272 [==============================] - 125s 460ms/step - loss: 0.6610\n",
            "Epoch 20/35\n",
            "272/272 [==============================] - 128s 470ms/step - loss: 0.6443\n",
            "Epoch 21/35\n",
            "272/272 [==============================] - 125s 461ms/step - loss: 0.6284\n",
            "Epoch 22/35\n",
            "272/272 [==============================] - 126s 464ms/step - loss: 0.6122\n",
            "Epoch 23/35\n",
            "272/272 [==============================] - 124s 456ms/step - loss: 0.5972\n",
            "Epoch 24/35\n",
            "272/272 [==============================] - 126s 462ms/step - loss: 0.5822\n",
            "Epoch 25/35\n",
            "272/272 [==============================] - 124s 457ms/step - loss: 0.5680\n",
            "Epoch 26/35\n",
            "272/272 [==============================] - 126s 462ms/step - loss: 0.5540\n",
            "Epoch 27/35\n",
            "272/272 [==============================] - 126s 462ms/step - loss: 0.5406\n",
            "Epoch 28/35\n",
            "272/272 [==============================] - 123s 453ms/step - loss: 0.5267\n",
            "Epoch 29/35\n",
            "272/272 [==============================] - 125s 460ms/step - loss: 0.5145\n",
            "Epoch 30/35\n",
            "272/272 [==============================] - 125s 459ms/step - loss: 0.5023\n",
            "Epoch 31/35\n",
            "272/272 [==============================] - 128s 469ms/step - loss: 0.4900\n",
            "Epoch 32/35\n",
            "272/272 [==============================] - 127s 467ms/step - loss: 0.4780\n",
            "Epoch 33/35\n",
            "272/272 [==============================] - 125s 461ms/step - loss: 0.4659\n",
            "Epoch 34/35\n",
            "272/272 [==============================] - 127s 465ms/step - loss: 0.4548\n",
            "Epoch 35/35\n",
            "272/272 [==============================] - 125s 460ms/step - loss: 0.4433\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4ecee9430>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save('model.h5') \n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Oljo-Gx6Opqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Defining inference models\n",
        "Create inference models for predicting answers.\n",
        "\n",
        "*Encoder inference model* : Takes the question as input and outputs LSTM states ( h and c ).\n",
        "\n",
        "*Decoder inference model* : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the <start> tag ). It will output the answers for the question which we fed to the encoder model and its state values."
      ],
      "metadata": {
        "id": "zOvT--1UGScw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "  \n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
        "  decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
        "  \n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  \n",
        "  decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "      decoder_embedding , initial_state=decoder_states_inputs)\n",
        "  decoder_states = [state_h, state_c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = tf.keras.models.Model(\n",
        "      [decoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  return encoder_model , decoder_model\n"
      ],
      "metadata": {
        "id": "pwqYD8xuGOlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Talking with Chatbot"
      ],
      "metadata": {
        "id": "RWF-cuGMGV0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens(sentence : str ):\n",
        "  sentence = re.sub(r'[^\\w\\s]','',sentence)\n",
        "  words = sentence.lower().split()\n",
        "  tokens_list = list()\n",
        "  for word in words:\n",
        "    try: \n",
        "      tokens_list.append(tokenizer.word_index[ word ]) \n",
        "    except KeyError:\n",
        "      print(\"I don't understand the word\", word)\n",
        "      return \n",
        "  return preprocessing.sequence.pad_sequences([tokens_list], maxlen=MAX_LEN , padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "e4klohR4GYjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10): \n",
        "    question = str_to_tokens( input( 'Enter question : ' ) )\n",
        "    if not isinstance(question, type(None)):\n",
        "      states_values = enc_model.predict(question)\n",
        "      empty_target_seq = np.zeros(( 1 , 1 ))\n",
        "      empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "      stop_condition = False\n",
        "      decoded_translation = ''\n",
        "      while not stop_condition :\n",
        "          dec_outputs, h, c = dec_model.predict([ empty_target_seq ] + states_values, verbose = 0)\n",
        "          sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "          sampled_word = None\n",
        "          for word , index in tokenizer.word_index.items() :\n",
        "              if sampled_word_index == index :\n",
        "                  decoded_translation += ' {}'.format(word)\n",
        "                  sampled_word = word\n",
        "          \n",
        "          if sampled_word == 'end' or len(decoded_translation.split()) > MAX_LEN:\n",
        "              stop_condition = True\n",
        "              \n",
        "          empty_target_seq = np.zeros(( 1 , 1 ))  \n",
        "          empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "          states_values = [ h , c ] \n",
        "    else: \n",
        "      break\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "metadata": {
        "id": "SvEiSecCGnRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1944358-576f-477b-8ce4-6481b5546444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : How many walks per day a Labrador needs?\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 30) for input KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, 1).\n",
            "WARNING:tensorflow:5 out of the last 56 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc4e225e550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " what is the dog end\n",
            "Enter question : How much space a dog needs?\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " what kind of dog do you have end\n",
            "Enter question : Should I bath my cat?\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            " well that's a good idea need me to help you with anything else end\n",
            "Enter question : What is an easy meal to make?\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            " the sound is that all set end\n",
            "Enter question : Can you set my alarm for 6 am tomorrow?\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " yes i can do that for you end\n",
            "Enter question : What to see in France?\n",
            "I don't understand the word france\n"
          ]
        }
      ]
    }
  ]
}